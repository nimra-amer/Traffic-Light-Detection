{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "166efbbf-bcad-4280-892a-1bbfdb70e39e",
   "metadata": {},
   "source": [
    "# << part 1 Data Scrapping>>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f2652-983f-4bf3-9541-367e77d1efaf",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4660156-b5a9-45fe-845e-eed2f2dea50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import urllib.request\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException ,  ElementClickInterceptedException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4358a046-9fe0-420f-9076-6668680aee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Service(ChromeDriverManager().install())\n",
    "options = Options()\n",
    "driver = Chrome(options, service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8134bede-b71c-4856-a7ee-e90a19816bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "urlPath = 'https://www.shutterstock.com/search/street-traffic-lights?image_type=photo'\n",
    "driver.get(urlPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0be63017-606a-4635-a242-b3c5f8c6cea5",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Scrolling the page till the end\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") \n",
    "time.sleep(5)      #Giving the time to fully load an image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2affa4fe-d90f-4968-88af-741fc913adc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extractng the images by class names\n",
    "images = driver.find_elements(By.CLASS_NAME , \"mui-t7xql4-a-inherit-link\")\n",
    "URL = []\n",
    "\n",
    "#Outer Loop : Used to move to the next page in case the first one has reached to its end\n",
    "for i in range(0,13): #accessing only 14 pages so that we will get approximately 1400 images \n",
    " try:\n",
    "   next_page = WebDriverWait(driver, 20).until(\n",
    "    EC.presence_of_element_located((By.XPATH, '//*[@id=\"__next\"]/div[3]/div/div/div[1]/div/div[6]/div[3]/div/div[1]/a'))   #accessing the \"NEXT\" button through XPATH\n",
    "    )\n",
    "   driver.execute_script(\"arguments[0].scrollIntoView();\", next_page) #To Avoid ElementClickInterceptedException\n",
    "   next_page.click()\n",
    "\n",
    " except TimeoutException:\n",
    "    driver.back() #IF no button is found , skip to the next page\n",
    "\n",
    " for img in images:     #Inner Loop for going through every image and getting their URL so that they can be downloaded\n",
    "    try:\n",
    "        img.click()   #first click on the image to go to the next page\n",
    "        \n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, \"mui-1jn9gxg-link-disabled\"))   #Waitng for the image on the next page to be loaded fully\n",
    "        )\n",
    "\n",
    "        image1 = driver.find_element(By.CLASS_NAME, \"mui-1jn9gxg-link-disabled\")  \n",
    "        image1.click()                        #Click on the loaded image\n",
    "\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"mui-1fxui0y-image\"))   #Waiting for the actual image to be fully loaded\n",
    "        )\n",
    "\n",
    "        actual_image = driver.find_element(By.CLASS_NAME, \"mui-1fxui0y-image\")   #Extracting the actual image\n",
    "        URL.append(actual_image.get_attribute('src'))      #Extracting the \"URL\" of the actual image that is written in \"src\" attribute\n",
    "    \n",
    "    \n",
    "        driver.back()  #Going back to the main page to repeat the process\n",
    "    \n",
    "    #Handling Exceptions\n",
    "    except TimeoutException:                                                      \n",
    "        driver.back()      #Skip to the next image if the element is not found after waiting for the image to be loaded by going back to the main page\n",
    "\n",
    "    except NoSuchElementException:\n",
    "        driver.back()       #Skip to the next image if the element is not found after waiting for the image to be loaded by going back to the main page\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5281750-eb6f-4d34-b6de-40a6730cea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading the images from the URL's and storing them in jpg format\n",
    "for i in range(0,len(URL)):    \n",
    "    urllib.request.urlretrieve(str(URL[i]),\"images/traffic_signals{}.jpg\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139dd7f5-2697-47ae-b797-9e0cddf1e53d",
   "metadata": {},
   "source": [
    "## Result of Selinium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bacc32f-9100-4ab8-a915-2edb3431373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(image_folder):\n",
    "    if filename.endswith(\".jpeg\") or filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        # Read the original image\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        original_image = cv2.imread(image_path)\n",
    "\n",
    "        # Determine orientation\n",
    "        scale_factor = 1.2\n",
    "        result_image = estimate_anchor_boxes(image_path, scale_factor)\n",
    "        # Display the original and augmented images\n",
    "        plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(\"Original Image\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1986f48d-d6b5-4a27-817e-4eb1131dd3e5",
   "metadata": {},
   "source": [
    "# <<  Part 2  Data Wrangling >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5666f-e426-4831-b266-efd84244e18a",
   "metadata": {},
   "source": [
    "##         Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "923cc2a8-75aa-41ab-bfc9-9eb1c52452d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-25 22:29:51.878284: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f476294-cfad-4e1a-b695-43d7e0c56dce",
   "metadata": {},
   "source": [
    "##  2.1 Data  Augmentation..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a33b6bb-8afc-4d34-9dc6-3b6fe507762a",
   "metadata": {},
   "source": [
    "##  2.1.1 Calculate the image orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a37a7323-c15b-43ec-969f-c2482682e3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_traffic_signal_orientation(image_path):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply GaussianBlur to reduce noise and improve contour detection\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)  #(5, 5): This tuple represents the size of the kernel used for blurring \n",
    "    #0: This parameter represents the standard deviation of the Gaussian kernel along the x and y directions\n",
    "\n",
    "    # Use Canny edge detection 50: This is the lower threshold for the Canny edge detector 150: This is the upper threshold for the Canny edge detector\n",
    "    edges = cv2.Canny(blurred, 50, 150)\n",
    "#Pixels with gradient magnitudes above this threshold are considered strong edges. The gap between the lower and upper thresholds is used to identify pixels that are part of weak edges\n",
    "\n",
    "    \n",
    "    # Find contours in the edged image\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Iterate through the contours and determine the aspect ratio\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        aspect_ratio = float(w) / h\n",
    "\n",
    "        # Adjust the aspect ratio threshold based on your dataset\n",
    "        if aspect_ratio > 1.5:\n",
    "            orientation = \"Horizontal\"\n",
    "        else:\n",
    "            orientation = \"Vertical\"\n",
    "\n",
    "        return orientation\n",
    "        #A bounding box is a rectangular box that encloses a region of interest or an object in an image.\n",
    "    #Contours are the continuous curves that form the boundary of an object in an image\n",
    "\n",
    "    return \"Unable to determine orientation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390a55c0-b5f1-47c3-b83b-8f1ecbf7e049",
   "metadata": {},
   "source": [
    "## 2.1.2 image Rotation based on orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77d47b9a-ddc9-48ed-bd2c-28ad6d204e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_orientation_aware_augmentation(image, orientation):\n",
    "    datagen = ImageDataGenerator() #s a class provided by Keras that allows you to perform various image augmentations\n",
    "\n",
    "    if orientation == 'Horizontal':\n",
    "        datagen.horizontal_flip = True\n",
    "        datagen.rotation_range = 10  # You can adjust the rotation range\n",
    "    elif orientation == 'Vertical':\n",
    "        datagen.vertical_flip = True\n",
    "        datagen.rotation_range = 10\n",
    "\n",
    "    # Reshape the image to (1, height, width, channels) as required by the flow method\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    # Generate augmented images\n",
    "    augmented_images = datagen.flow(image, batch_size=1)\n",
    "\n",
    "    # Retrieve the first augmented image\n",
    "    augmented_image = augmented_images.next()[0].astype('uint8')\n",
    "\n",
    "    return augmented_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bbfca8-8513-4263-939b-a2661dcdc701",
   "metadata": {},
   "source": [
    "## 2.1.3 Increase Brightness and contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf1b8e7a-47fc-4ca4-b4f4-a4f02f67b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_brightness_contrast(image, brightness_factor=1.0, contrast_factor=1.0):\n",
    "    # Convert the image to a float\n",
    "    image_float = image.astype(float)\n",
    "\n",
    "    # Adjust brightness\n",
    "    image_float *= brightness_factor\n",
    "\n",
    "    # Adjust contrast\n",
    "    image_float = (image_float - 128) * contrast_factor + 128\n",
    "\n",
    "    # Clip values to be in the valid range [0, 255]\n",
    "    image_float = np.clip(image_float, 0, 255)  \n",
    "\n",
    "    # Convert back to uint8\n",
    "    adjusted_image = image_float.astype(np.uint8)\n",
    "\n",
    "    return adjusted_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db813dd-0fc8-42b6-96da-42ac5b9d5649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12a0944f-a5b9-433a-a898-dd21a133f69b",
   "metadata": {},
   "source": [
    "## 2.1.3 Apply Zooming techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b334b5e-36ec-4313-b030-ed53457af8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_orientation_aware_zoom(image, orientation):\n",
    "    zoom_factor = np.random.uniform(1.2, 1.5)  # Adjust as needed\n",
    "\n",
    "    if orientation == 'Vertical':\n",
    "        # Apply zooming more along the vertical axis\n",
    "        zoomed_image = cv2.resize(image, None, fx=1, fy=zoom_factor)\n",
    "    elif orientation == 'Horizontal':\n",
    "        # Apply zooming more along the horizontal axis\n",
    "        zoomed_image = cv2.resize(image, None, fx=zoom_factor, fy=1)\n",
    "    else:\n",
    "        # Apply zooming with equal scaling for other orientations\n",
    "        zoomed_image = cv2.resize(image, None, fx=zoom_factor, fy=zoom_factor)\n",
    "    return zoomed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02b6ceb-10a4-445b-827d-259e2d04d64b",
   "metadata": {},
   "source": [
    "## 2.1 Result of Augumented Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d435bb-9f3a-4c9c-b165-1d278397164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"Desktop/images\"\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for filename in os.listdir(image_folder):\n",
    "    if filename.endswith(\".jpeg\") or filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        # Read the original image\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        original_image = cv2.imread(image_path)\n",
    "\n",
    "        # Determine orientation\n",
    "        orientation = determine_traffic_signal_orientation(image_path)\n",
    "\n",
    "        # Apply orientation-aware augmentation\n",
    "        augmented_image = apply_orientation_aware_augmentation(original_image, orientation)\n",
    "\n",
    "        # Adjust brightness and contrast\n",
    "        brightness_factor = 1.5  # Adjust as needed\n",
    "        contrast_factor = 1.2    # Adjust as needed\n",
    "        adjusted_image = adjust_brightness_contrast(augmented_image, brightness_factor, contrast_factor)\n",
    "\n",
    "        # Apply orientation-aware augmentation again (if needed)\n",
    "        Zoomed_image = apply_orientation_aware_augmentation(adjusted_image, orientation)\n",
    "        plt.figure(figsize=(15, 7))\n",
    "        # Display the original and augmented images\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(\"Original Image\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(cv2.cvtColor(Zoomed_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(\"Augmented Image\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9e4ae8-f6ed-4542-9692-fdd0456cd0d5",
   "metadata": {},
   "source": [
    "## 2.2  normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7118ef33-f718-4fb7-ac39-9a8ad92b4bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_normalization(image, color_space=cv2.COLOR_BGR2LAB):\n",
    "    # Pixel normalization with scaling\n",
    "    normalized_image = ((image.astype(np.float32) / 255.0) * 255).astype(np.uint8)\n",
    "    normalized_image = cv2.cvtColor(normalized_image, color_space)\n",
    "    return normalized_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0389837-e5b1-4510-8834-afc24fa209b7",
   "metadata": {},
   "source": [
    "## Result of NOrmalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac50d39-93ff-4588-acb2-f308f425a523",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for filename in os.listdir(image_folder):\n",
    "    if filename.endswith(\".jpeg\") or filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        # Read the original image\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        original_image = cv2.imread(image_path)\n",
    "\n",
    "        # Determine orientation\n",
    "        normalized_pixel_image = pixel_normalization(original_image)\n",
    "        plt.figure(figsize=(15, 7))\n",
    "        # Display the original and augmented images\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(\"Original Image\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(cv2.cvtColor(normalized_pixel_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(\"normalized_pixel_image\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29f7663-101a-4227-990e-fd6f63d9d2f4",
   "metadata": {},
   "source": [
    "## 2.3 Anchor Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46336925-4c62-4eb4-918a-04858f2cde8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_anchor_boxes(image_path,  scale_factor):\n",
    "    # Read the color image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Apply edge detection on the color image\n",
    "    edges = cv2.Canny(image, 50, 150)\n",
    "\n",
    "    # Find contours in the edge-detected image contours are simply the boundaries of an object with the same intensity or color in an image.\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Extract bounding boxes directly from contours\n",
    "    bounding_boxes = [cv2.boundingRect(contour) for contour in contours]\n",
    "\n",
    "    # Use the specified number of bounding boxes or the available ones if fewer sorts the boxes in descending order, based on the product of width and height of each bounding box \n",
    "    selected_boxes = sorted(bounding_boxes, key=lambda x: x[2] * x[3], reverse=True)\n",
    "    num_boxes = min(3, len(selected_boxes))\n",
    "    \n",
    "    # Take the top 'num_boxes' boxes\n",
    "    selected_boxes = selected_boxes[:num_boxes]\n",
    "\n",
    "    # Scale the selected anchor boxes by the desired scale factor\n",
    "    scaled_anchor_boxes = (np.array(selected_boxes) * scale_factor).astype(int)\n",
    "\n",
    "    # Visualize the anchor boxes on the original image  (x, y): The coordinates of the top-left corner of the rectangle. (x + w, y + h): The coordinates of the bottom-right corner of the rectangle\n",
    "    image_with_boxes = image.copy()\n",
    "    for box in scaled_anchor_boxes:\n",
    "        x, y, w, h = box\n",
    "        cv2.rectangle(image_with_boxes, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    return image_with_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbfd5e3-3bf0-40c6-81d4-7f099e53082c",
   "metadata": {},
   "source": [
    "## Result of Anchor BOxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39704e2b-36b7-417c-b091-850662d4d949",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(image_folder):\n",
    "    if filename.endswith(\".jpeg\") or filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        # Read the original image\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        original_image = cv2.imread(image_path)\n",
    "\n",
    "        # Determine orientation\n",
    "        scale_factor = 1.2\n",
    "        result_image = estimate_anchor_boxes(image_path, scale_factor)\n",
    "        # Display the original and augmented images\n",
    "        plt.figure(figsize=(15, 7))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(\"Original Image\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(\"Augmented Image\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74361d67-8ac8-4ace-91f2-87c637c6717c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
